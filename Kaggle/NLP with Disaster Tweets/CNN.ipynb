{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Test data...\n",
      "Reading Training data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Test data...\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "print(\"Reading Training data...\")\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = train_df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has 7613 rows and 5 columns\n",
      "The test data has 3263 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The training data has {} rows and {} columns\".format(train_df.shape[0], train_df.shape[1]))\n",
    "print(\"The test data has {} rows and {} columns\".format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of empty cells in training data:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "amount of empty cells in test data:\n",
      "id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"amount of empty cells in training data:\")\n",
    "print(train_df.isna().sum())\n",
    "print(\"\")\n",
    "print(\"amount of empty cells in test data:\")\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Bag of Words\n",
    "NLP Steps:\n",
    "1. Lower words and remove punctuation\n",
    "2. Expand contradictions\n",
    "3. Lemmatizing\n",
    "4. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rockyfire update  california hwy 20 closed in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im on top of the hill and i can see a fire in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theres an emergency evacuation happening now i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im afraid that the tornado is coming to our area</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  all residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n",
       "5   8     NaN      NaN  rockyfire update  california hwy 20 closed in ...   \n",
       "6  10     NaN      NaN  flood disaster heavy rain causes flash floodin...   \n",
       "7  13     NaN      NaN  im on top of the hill and i can see a fire in ...   \n",
       "8  14     NaN      NaN  theres an emergency evacuation happening now i...   \n",
       "9  15     NaN      NaN   im afraid that the tornado is coming to our area   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text: str):\n",
    "    text = text.lower()\n",
    "    text_nopunct = \"\".join([character for character in text if character not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_punct(x))\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rockyfire update  california hwy 20 closed in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im on top of the hill and i can see a fire in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theres an emergency evacuation happening now i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im afraid that the tornado is coming to our area</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  all residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n",
       "5   8     NaN      NaN  rockyfire update  california hwy 20 closed in ...   \n",
       "6  10     NaN      NaN  flood disaster heavy rain causes flash floodin...   \n",
       "7  13     NaN      NaN  im on top of the hill and i can see a fire in ...   \n",
       "8  14     NaN      NaN  theres an emergency evacuation happening now i...   \n",
       "9  15     NaN      NaN   im afraid that the tornado is coming to our area   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expand_contractions(s, contractions=contractions):\n",
    "    def replace(match):\n",
    "         return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: expand_contractions(x))\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deed are the reason of this earthquake may...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all resident asked to shelter in place are bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska a sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rockyfire update california hwy 20 closed in b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im on top of the hill and i can see a fire in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there an emergency evacuation happening now in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im afraid that the tornado is coming to our area</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deed are the reason of this earthquake may...   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  all resident asked to shelter in place are bei...   \n",
       "3   6     NaN      NaN  13000 people receive wildfire evacuation order...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby alaska a sm...   \n",
       "5   8     NaN      NaN  rockyfire update california hwy 20 closed in b...   \n",
       "6  10     NaN      NaN  flood disaster heavy rain cause flash flooding...   \n",
       "7  13     NaN      NaN  im on top of the hill and i can see a fire in ...   \n",
       "8  14     NaN      NaN  there an emergency evacuation happening now in...   \n",
       "9  15     NaN      NaN   im afraid that the tornado is coming to our area   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text: str):\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    lemmatize_text = \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "    \n",
    "    return lemmatize_text\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: lemmatize(x))\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rockyfire update california hwy 20 closed dire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im top hill see fire wood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>emergency evacuation happening building across...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im afraid tornado coming area</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN         deed reason earthquake may allah forgive u   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  resident asked shelter place notified officer ...   \n",
       "3   6     NaN      NaN  13000 people receive wildfire evacuation order...   \n",
       "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfire pour...   \n",
       "5   8     NaN      NaN  rockyfire update california hwy 20 closed dire...   \n",
       "6  10     NaN      NaN  flood disaster heavy rain cause flash flooding...   \n",
       "7  13     NaN      NaN                          im top hill see fire wood   \n",
       "8  14     NaN      NaN  emergency evacuation happening building across...   \n",
       "9  15     NaN      NaN                      im afraid tornado coming area   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove Stopwords\n",
    "stopwords = set(stopwords.words('english')) \n",
    "def remove_stopwords(text: str):\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    text_no_sw = \" \".join([w for w in text if w not in stopwords])\n",
    "    \n",
    "    return text_no_sw\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: remove_stopwords(x))\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = train_df[\"text\"]\n",
    "y = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(text_raw)\n",
    "\n",
    "max_len = 150\n",
    "\n",
    "x_raw = tokenizer.texts_to_sequences(text_raw)\n",
    "x = tokenizer.texts_to_sequences(x_text)\n",
    "\n",
    "x_raw = tf.keras.preprocessing.sequence.pad_sequences(x_raw, maxlen = max_len, padding = \"post\", truncating = \"post\", value = 0)\n",
    "x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen = max_len, padding = \"post\", truncating = \"post\", value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21259\n"
     ]
    }
   ],
   "source": [
    "len_vocab = len(tokenizer.word_index) + 1\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "print(len_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 150, 300)     6377700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 149, 200)     120200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 148, 200)     180200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 147, 200)     240200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 146, 200)     300200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 145, 200)     360200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 144, 200)     420200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1200)         0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1200)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          614912      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,778,165\n",
      "Trainable params: 8,778,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input((max_len,))\n",
    "\n",
    "l = layers.Embedding(input_dim = len_vocab, output_dim = 300)(inputs)\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5,6,7]\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    conv = layers.Conv1D(filters = 200, kernel_size = filter_size, activation = \"relu\")(l)\n",
    "    pool = layers.GlobalMaxPool1D()(conv)\n",
    "    \n",
    "    convs.append(pool)\n",
    "\n",
    "merge = layers.concatenate(convs)\n",
    "l = layers.Dropout(0.1)(merge)\n",
    "l = layers.Dense(512, activation = \"relu\")(l)\n",
    "l = layers.Dropout(0.1)(l)\n",
    "l = layers.Dense(256, activation = \"relu\")(l)\n",
    "l = layers.Dropout(0.1)(l)\n",
    "l = layers.Dense(128, activation = \"relu\")(l)\n",
    "output = layers.Dense(1, activation = \"sigmoid\")(l)\n",
    "\n",
    "model = keras.Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/100000\n",
      "6851/6851 [==============================] - 97s 14ms/sample - loss: 0.6829 - acc: 0.5735 - val_loss: 0.6824 - val_acc: 0.5341\n",
      "Epoch 2/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.6094 - acc: 0.6787 - val_loss: 0.5704 - val_acc: 0.7375\n",
      "Epoch 3/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.3492 - acc: 0.8748 - val_loss: 0.5202 - val_acc: 0.7703\n",
      "Epoch 4/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.1569 - acc: 0.9451 - val_loss: 0.6110 - val_acc: 0.7690\n",
      "Epoch 5/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0648 - acc: 0.9766 - val_loss: 0.7445 - val_acc: 0.7769\n",
      "Epoch 6/100000\n",
      "6851/6851 [==============================] - 91s 13ms/sample - loss: 0.0355 - acc: 0.9889 - val_loss: 0.8828 - val_acc: 0.7546\n",
      "Epoch 7/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0241 - acc: 0.9923 - val_loss: 0.9194 - val_acc: 0.7664\n",
      "Epoch 8/100000\n",
      "6851/6851 [==============================] - 91s 13ms/sample - loss: 0.0213 - acc: 0.9945 - val_loss: 0.9423 - val_acc: 0.7664\n",
      "Epoch 9/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0178 - acc: 0.9956 - val_loss: 0.9425 - val_acc: 0.7559\n",
      "Epoch 10/100000\n",
      "6851/6851 [==============================] - 96s 14ms/sample - loss: 0.0202 - acc: 0.9947 - val_loss: 0.8656 - val_acc: 0.7546\n",
      "Epoch 11/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0169 - acc: 0.9950 - val_loss: 0.8206 - val_acc: 0.7572\n",
      "Epoch 12/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.8458 - val_acc: 0.7572\n",
      "Epoch 13/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0120 - acc: 0.9955 - val_loss: 0.8576 - val_acc: 0.7730\n",
      "Epoch 14/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0127 - acc: 0.9953 - val_loss: 0.9059 - val_acc: 0.7546\n",
      "Epoch 15/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0119 - acc: 0.9947 - val_loss: 0.8854 - val_acc: 0.7756\n",
      "Epoch 16/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0133 - acc: 0.9950 - val_loss: 0.9075 - val_acc: 0.7415\n",
      "Epoch 17/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0124 - acc: 0.9943 - val_loss: 0.8418 - val_acc: 0.7769\n",
      "Epoch 18/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0109 - acc: 0.9950 - val_loss: 0.8700 - val_acc: 0.7559\n",
      "Epoch 19/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0112 - acc: 0.9945 - val_loss: 0.8540 - val_acc: 0.7612\n",
      "Epoch 20/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0088 - acc: 0.9958 - val_loss: 0.8696 - val_acc: 0.7625\n",
      "Epoch 21/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0107 - acc: 0.9947 - val_loss: 0.8959 - val_acc: 0.7625\n",
      "Epoch 22/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0094 - acc: 0.9952 - val_loss: 0.8746 - val_acc: 0.7625\n",
      "Epoch 23/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0094 - acc: 0.9949 - val_loss: 0.8745 - val_acc: 0.7664\n",
      "Epoch 24/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0093 - acc: 0.9946 - val_loss: 0.8990 - val_acc: 0.7585\n",
      "Epoch 25/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0082 - acc: 0.9956 - val_loss: 0.9157 - val_acc: 0.7598\n",
      "Epoch 26/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0075 - acc: 0.9956 - val_loss: 0.9663 - val_acc: 0.7585\n",
      "Epoch 27/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0084 - acc: 0.9952 - val_loss: 0.9836 - val_acc: 0.7717\n",
      "Epoch 28/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0088 - acc: 0.9950 - val_loss: 0.9571 - val_acc: 0.7612\n",
      "Epoch 29/100000\n",
      "6851/6851 [==============================] - 95s 14ms/sample - loss: 0.0090 - acc: 0.9943 - val_loss: 0.9124 - val_acc: 0.7612\n",
      "Epoch 30/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0084 - acc: 0.9956 - val_loss: 0.9117 - val_acc: 0.7638\n",
      "Epoch 31/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0081 - acc: 0.9958 - val_loss: 1.0064 - val_acc: 0.7480\n",
      "Epoch 32/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0078 - acc: 0.9955 - val_loss: 0.9920 - val_acc: 0.7677\n",
      "Epoch 33/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0078 - acc: 0.9949 - val_loss: 1.0153 - val_acc: 0.7598\n",
      "Epoch 34/100000\n",
      "6851/6851 [==============================] - 95s 14ms/sample - loss: 0.0076 - acc: 0.9950 - val_loss: 1.0174 - val_acc: 0.7703\n",
      "Epoch 35/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0072 - acc: 0.9952 - val_loss: 1.0554 - val_acc: 0.7664\n",
      "Epoch 36/100000\n",
      "6851/6851 [==============================] - 96s 14ms/sample - loss: 0.0072 - acc: 0.9959 - val_loss: 1.0580 - val_acc: 0.7664\n",
      "Epoch 37/100000\n",
      "6851/6851 [==============================] - 96s 14ms/sample - loss: 0.0074 - acc: 0.9958 - val_loss: 1.0521 - val_acc: 0.7625\n",
      "Epoch 38/100000\n",
      "6851/6851 [==============================] - 95s 14ms/sample - loss: 0.0077 - acc: 0.9949 - val_loss: 1.1048 - val_acc: 0.7415\n",
      "Epoch 39/100000\n",
      "6851/6851 [==============================] - 98s 14ms/sample - loss: 0.0070 - acc: 0.9953 - val_loss: 1.0745 - val_acc: 0.7664\n",
      "Epoch 40/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0070 - acc: 0.9959 - val_loss: 1.0945 - val_acc: 0.7651\n",
      "Epoch 41/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0072 - acc: 0.9952 - val_loss: 1.1244 - val_acc: 0.7651\n",
      "Epoch 42/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0066 - acc: 0.9959 - val_loss: 1.1453 - val_acc: 0.7651\n",
      "Epoch 43/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0067 - acc: 0.9958 - val_loss: 1.1859 - val_acc: 0.7664\n",
      "Epoch 44/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0067 - acc: 0.9955 - val_loss: 1.2298 - val_acc: 0.7651\n",
      "Epoch 45/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0065 - acc: 0.9955 - val_loss: 1.2271 - val_acc: 0.7664\n",
      "Epoch 46/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0066 - acc: 0.9961 - val_loss: 1.2447 - val_acc: 0.7651\n",
      "Epoch 47/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0066 - acc: 0.9959 - val_loss: 1.2656 - val_acc: 0.7651\n",
      "Epoch 48/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0063 - acc: 0.9958 - val_loss: 1.2739 - val_acc: 0.7664\n",
      "Epoch 49/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0066 - acc: 0.9962 - val_loss: 1.2733 - val_acc: 0.7677\n",
      "Epoch 50/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0068 - acc: 0.9956 - val_loss: 1.2581 - val_acc: 0.7638\n",
      "Epoch 51/100000\n",
      "6851/6851 [==============================] - 92s 13ms/sample - loss: 0.0065 - acc: 0.9964 - val_loss: 1.2609 - val_acc: 0.7598\n",
      "Epoch 52/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0066 - acc: 0.9965 - val_loss: 1.3341 - val_acc: 0.7651\n",
      "Epoch 53/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0065 - acc: 0.9959 - val_loss: 1.3425 - val_acc: 0.7664\n",
      "Epoch 54/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0067 - acc: 0.9959 - val_loss: 1.3303 - val_acc: 0.7677\n",
      "Epoch 55/100000\n",
      "6851/6851 [==============================] - 94s 14ms/sample - loss: 0.0065 - acc: 0.9959 - val_loss: 1.2761 - val_acc: 0.7651\n",
      "Epoch 56/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0065 - acc: 0.9958 - val_loss: 1.2808 - val_acc: 0.7651\n",
      "Epoch 57/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0065 - acc: 0.9964 - val_loss: 1.3177 - val_acc: 0.7651\n",
      "Epoch 58/100000\n",
      "6851/6851 [==============================] - 93s 14ms/sample - loss: 0.0064 - acc: 0.9956 - val_loss: 1.3530 - val_acc: 0.7664\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"acc\"])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = \"loss\", patience = 10, mode = \"auto\")\n",
    "\n",
    "history = model.fit(x, y,\n",
    "                   batch_size = 1000,\n",
    "                   callbacks = [callback],\n",
    "                   epochs = 100000,\n",
    "                   validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-Score for prediction the training data is 0.9688454489920588\n"
     ]
    }
   ],
   "source": [
    "#Predict the train data\n",
    "pred_train = model.predict(x)\n",
    "\n",
    "pred_train[pred_train < 0.5] = 0\n",
    "pred_train[pred_train >= 0.5] = 1\n",
    "print(\"The F1-Score for prediction the training data is {}\".format(f1_score(y, pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_punct(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: expand_contractions(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: lemmatize(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "x_test_text = test_df[\"text\"]\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(x_test_text)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen = max_len, padding = \"post\", truncating = \"post\", value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(x_test)\n",
    "\n",
    "predict[predict < 0.5] = 0\n",
    "predict[predict >= 0.5] = 1\n",
    "predict = predict.astype(int)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2069\n",
       "1    1194\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.concat([pd.DataFrame(test_df[\"id\"]), pd.DataFrame(predict, columns = [\"target\"])], axis = 1)\n",
    "\n",
    "submission[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
